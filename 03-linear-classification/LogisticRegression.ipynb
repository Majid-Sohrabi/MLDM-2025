{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUCk6WmetDF6"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "In this seminar you will implement a logistic regression and train it using stochastic gradient descent modiffications, numpy and your brain."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lk8rFawEtDF7"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/Majid-Sohrabi/MLDM-2025/raw/refs/heads/main/03-linear-classification/dataset_not_scaled.pkl .\n",
        "!wget https://github.com/Majid-Sohrabi/MLDM-2025/raw/refs/heads/main/03-linear-classification/dataset_scaled.pkl ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PZ3u9kKOtDGC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s35ubHY0tDGG"
      },
      "source": [
        "### Two-dimensional classification problem\n",
        "\n",
        "To make things more intuitive, let's solve a 2D classification problem with syntetic data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "78npydYotDGH"
      },
      "outputs": [],
      "source": [
        "#https://docs.python.org/3/library/pickle.html\n",
        "import pickle\n",
        "\n",
        "with open(\"dataset_scaled.pkl\", \"rb\") as f:\n",
        "    X, y = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(y)"
      ],
      "metadata": {
        "id": "VAscR52eS6Qr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"X[:3]:\\n{X[:3]}\\ny[:3]:\\n{y[:3]}\")"
      ],
      "metadata": {
        "id": "2tagNjsWTQwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Dwyt1Z-tDGK"
      },
      "outputs": [],
      "source": [
        "import matplotlib as mpl\n",
        "cmap = mpl.colormaps['Paired']\n",
        "colors = dict(zip(np.unique(y), cmap(np.linspace(0, 1, len(np.unique(y))))))\n",
        "\n",
        "for y_class in np.unique(y):\n",
        "  plt.scatter(X[:, 0][y == y_class], X[:, 1][y == y_class], color=colors[y_class], label=y_class)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x358yQoJtDGV"
      },
      "source": [
        "\n",
        "Since the problem above isn't linearly separable, we add quadratic features to the classifier.\n",
        "\n",
        "Implement this transformation in the __expand__ function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vomlUyUJtDGW"
      },
      "outputs": [],
      "source": [
        "def expand(X):\n",
        "    \"\"\"\n",
        "    Adds quadratic features.\n",
        "    This function allows your linear model to make non-linear separation.\n",
        "\n",
        "    For each sample (row in matrix), compute an expanded row:\n",
        "    [feature0, feature1, feature0^2, feature1^2, feature1*feature2, 1]\n",
        "\n",
        "    :param X: matrix of features, shape [n_samples,2]\n",
        "    :returns: expanded features of shape [n_samples,6]\n",
        "    \"\"\"\n",
        "    X_0 = X[:,0]\n",
        "    X_1 = X[:,1]\n",
        "\n",
        "    X_expanded =  #<YOUR CODE>\n",
        "\n",
        "    return X_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sCakZJfEtDGb"
      },
      "outputs": [],
      "source": [
        "#simple test on random numbers\n",
        "#[all 8 random numbers are 100% random :P]\n",
        "dummy_X = np.array([\n",
        "        [0,0],\n",
        "        [1,0],\n",
        "        [2.61,-1.28],\n",
        "        [-0.59,2.1]\n",
        "    ])\n",
        "\n",
        "#call your expand function\n",
        "dummy_expanded = expand(dummy_X)\n",
        "\n",
        "#what it should have returned:   x0       x1       x0^2     x1^2     x0*x1    1\n",
        "dummy_expanded_ans = np.array([[ 0.    ,  0.    ,  0.    ,  0.    ,  0.    ,  1.    ],\n",
        "                               [ 1.    ,  0.    ,  1.    ,  0.    ,  0.    ,  1.    ],\n",
        "                               [ 2.61  , -1.28  ,  6.8121,  1.6384, -3.3408,  1.    ],\n",
        "                               [-0.59  ,  2.1   ,  0.3481,  4.41  , -1.239 ,  1.    ]])\n",
        "\n",
        "#tests\n",
        "assert isinstance(dummy_expanded,np.ndarray), \"please make sure you return numpy array\"\n",
        "assert dummy_expanded.shape==dummy_expanded_ans.shape, \"please make sure your shape is correct\"\n",
        "assert np.allclose(dummy_expanded,dummy_expanded_ans,1e-3), \"Something's out of order with features\"\n",
        "\n",
        "print(\"Seems legit!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZP-MWZsatDGf"
      },
      "source": [
        "### Logistic regression\n",
        "Now, let's write function `classify` that predicts class given X as in logistic regression.\n",
        "\n",
        "The math should look like this:\n",
        "\n",
        "$$ P(y| \\vec x, \\vec w) = \\sigma(\\vec x \\cdot \\vec w )$$\n",
        "\n",
        "where x represents features, w are weights and $$\\sigma(a) = {1 \\over {1+e^{-a}}}$$\n",
        "\n",
        "We shall omit $ \\vec {arrows} $ in further formulae for simplicity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoQUYwnFuiA2"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n",
        "  return 1 / (1 + np.exp(-x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RgAXUBXouq4s"
      },
      "outputs": [],
      "source": [
        "logits = np.linspace(-10, 10, 101)\n",
        "plt.plot(logits, sigmoid(logits));"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9naNARLutDGh"
      },
      "outputs": [],
      "source": [
        "def classify(X, w):\n",
        "    \"\"\"\n",
        "    Given input features and weights\n",
        "    return predicted probabilities of y==1 given x, P(y=1|x), see description above\n",
        "\n",
        "    __don't forget to expand X inside classify and other functions__\n",
        "\n",
        "    :param X: feature matrix X of shape [n_samples,2] (non-exanded)\n",
        "    :param w: weight vector w of shape [6] for each of the expanded features\n",
        "    :returns: an array of predicted probabilities in [0,1] interval.\n",
        "    \"\"\"\n",
        "\n",
        "    return  #<YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f1lsbjXtDGl"
      },
      "outputs": [],
      "source": [
        "#sample usage / test just as the previous one\n",
        "dummy_weights = np.linspace(-1,1,6)\n",
        "\n",
        "dummy_probs = classify(dummy_X, dummy_weights)\n",
        "\n",
        "dummy_answers = np.array([ 0.73105858,  0.450166  ,  0.02020883,  0.59844257])\n",
        "\n",
        "assert isinstance(dummy_probs,np.ndarray), \"please return np.array\"\n",
        "assert dummy_probs.shape == dummy_answers.shape, \"please return an 1-d vector with answers for each object\"\n",
        "assert np.allclose(dummy_probs,dummy_answers,1e-3), \"There's something non-canonic about how probabilties are computed\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVJYTUiZtDGo"
      },
      "source": [
        "The loss you should try to minimize is the Logistic Loss aka crossentropy aka negative log-likelihood:\n",
        "\n",
        "$$ L =  - {1 \\over N} \\sum_i {y \\cdot log P(y|x,w) + (1-y) \\cdot log (1-P(y|x,w))}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ob9q9ReCDwB"
      },
      "source": [
        "Let's implement this loss inside compute_loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ync9b2U_tDGq"
      },
      "outputs": [],
      "source": [
        "def compute_loss(X, y, w):\n",
        "    \"\"\"\n",
        "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/0,\n",
        "    and weight vector w [6], compute scalar loss function using formula above.\n",
        "    \"\"\"\n",
        "    #<YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuDYEWVttDGu"
      },
      "outputs": [],
      "source": [
        "dummy_y = np.array([0,1,0,1])\n",
        "dummy_loss = compute_loss(dummy_X,dummy_y,dummy_weights)\n",
        "\n",
        "assert np.allclose(dummy_loss,0.66131), \"something wrong with loss\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k95NppGotDGz"
      },
      "source": [
        "Since we train our model with gradient descent, we gotta compute gradients.\n",
        "\n",
        "To be specific, we need a derivative of loss function over each weight [6 of them].\n",
        "\n",
        "$$ \\nabla L = {\\partial L \\over \\partial w} = ...$$\n",
        "\n",
        "Let us figure out a derivative with pen and paper and implement the `compute_grad` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgyNz8FztDG0"
      },
      "outputs": [],
      "source": [
        "def compute_grad(X, y, w):\n",
        "    \"\"\"\n",
        "    Given feature matrix X [n_samples,2], target vector [n_samples] of +1/0,\n",
        "    and weight vector w [6], compute vector [6] of derivatives of L over each weights.\n",
        "    \"\"\"\n",
        "    return  #<YOUR CODE>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "486jKv54tDG4"
      },
      "outputs": [],
      "source": [
        "#tests\n",
        "dummy_grads = compute_grad(dummy_X,dummy_y,dummy_weights)\n",
        "\n",
        "#correct answers in canonic form\n",
        "dummy_grads_ans = np.array([-0.06504252, -0.21728448, -0.1379879 , -0.43443953,  0.107504  , -0.05003101])\n",
        "\n",
        "assert isinstance(dummy_grads,np.ndarray)\n",
        "assert dummy_grads.shape == (6,), \"must return a vector of gradients for each weight\"\n",
        "assert len(set(np.round(dummy_grads/dummy_grads_ans,3))), \"gradients are wrong\"\n",
        "assert np.allclose(dummy_grads,dummy_grads_ans,1e-3), \"gradients are off by a coefficient\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fa3vheSmtDG9"
      },
      "source": [
        "Here's an auxiliary function that visualizes the predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrzGJMHRtDG_"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "def visualize(X, y, w, history):\n",
        "    plt.figure(figsize=(12,6))\n",
        "    \"\"\"draws classifier prediction with matplotlib magic\"\"\"\n",
        "    Z = classify(np.c_[xx.ravel(), yy.ravel()], w)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.contourf(xx, yy, Z, alpha=0.8)\n",
        "    plt.colorbar()\n",
        "    for y_class in np.unique(y):\n",
        "        plt.scatter(X[:, 0][y == y_class], X[:, 1][y == y_class], color=colors[y_class], label=y_class)\n",
        "    plt.legend()\n",
        "    plt.xlim(xx.min(), xx.max())\n",
        "    plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(history)\n",
        "    plt.grid()\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.xlabel(\"Iteration\")\n",
        "    ymin, ymax = plt.ylim()\n",
        "    plt.ylim(0, ymax)\n",
        "    display.clear_output(wait=True)\n",
        "    plt.show()\n",
        "    print('Accuracy =', np.mean((classify(X, w) >=0.5).astype('int') == y))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RS0icgahtDHF"
      },
      "outputs": [],
      "source": [
        "visualize(X, y, dummy_weights, [1, 0.5, 0.25],)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYpiYJs5tDHI"
      },
      "source": [
        "### Training\n",
        "In this section, we'll use the functions you wrote to train our classifier using stochastic gradient descent.\n",
        "\n",
        "Try to find an optimal learning rate for gradient descent for the given batch size.\n",
        "\n",
        "**Don't change the batch size!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYSYLPUFtDHI"
      },
      "outputs": [],
      "source": [
        "np.random.seed(2025)\n",
        "w = np.array([0,0,0,0,0,1])\n",
        "\n",
        "\n",
        "alpha = 0.1\n",
        "\n",
        "n_iter = 50\n",
        "batch_size = 4\n",
        "loss = np.zeros(n_iter)\n",
        "plt.figure(figsize=(12,5))\n",
        "for i in range(n_iter):\n",
        "    ind = np.random.choice(X.shape[0], batch_size)\n",
        "    loss[i] = compute_loss(X, y, w)\n",
        "    visualize(X[ind,:], y[ind], w, loss)\n",
        "\n",
        "    w = w - alpha * compute_grad(X[ind,:], y[ind], w)\n",
        "\n",
        "visualize(X, y, w, loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBF1k_R5tDHL"
      },
      "source": [
        "## Now, let's see what is happening, when we do not normalise features first.\n",
        "## What do you think will happen?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d_Yeia8OtDHN"
      },
      "outputs": [],
      "source": [
        "with open(\"dataset_not_scaled.pkl\", \"rb\") as f:\n",
        "    X, y = pickle.load(f)\n",
        "\n",
        "for y_class in np.unique(y):\n",
        "  plt.scatter(X[:, 0][y == y_class], X[:, 1][y == y_class], color=colors[y_class], label=y_class)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WMQnOJWdtDHR"
      },
      "outputs": [],
      "source": [
        "# Set parameters to show plots nicely\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 50), np.arange(y_min, y_max, h))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY4fmHNttDHU"
      },
      "outputs": [],
      "source": [
        "w = np.array([0,0,0,0,0,1])\n",
        "\n",
        "\n",
        "alpha = 0.0001\n",
        "\n",
        "n_iter = 50\n",
        "batch_size = 4\n",
        "loss = np.zeros(n_iter)\n",
        "plt.figure(figsize=(12,5))\n",
        "for i in range(n_iter):\n",
        "    ind = np.random.choice(X.shape[0], batch_size)\n",
        "    loss[i] = compute_loss(X, y, w)\n",
        "    visualize(X[ind,:], y[ind], w, loss)\n",
        "\n",
        "    w = w - alpha * compute_grad(X[ind,:], y[ind], w)\n",
        "\n",
        "visualize(X, y, w, loss)\n",
        "plt.clf()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JjvYCJIWI4cS"
      },
      "outputs": [],
      "source": [
        "loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "miLmUGyNtDH9"
      },
      "source": [
        "\n",
        "\n",
        "# Momentum SGD\n",
        "\n",
        "Let's implementi __momentum SGD__ as described [here](https://distill.pub/2017/momentum/).\n",
        "\n",
        "Find alpha & beta that results in fastest convergence rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9evy0dqtDH-"
      },
      "outputs": [],
      "source": [
        "#back to scaled data\n",
        "with open(\"dataset_scaled.pkl\", \"rb\") as f:\n",
        "    X, y = pickle.load(f)\n",
        "h = 0.01\n",
        "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "w = np.array([0,0,0,0,0,1])\n",
        "z = np.array([0,0,0,0,0,0])\n",
        "\n",
        "alpha = #<YOUR CODE>\n",
        "beta = #<YOUR CODE>\n",
        "\n",
        "\n",
        "n_iter = 50\n",
        "batch_size = 4\n",
        "loss = np.zeros(n_iter)\n",
        "plt.figure(figsize=(12,5))\n",
        "for i in range(n_iter):\n",
        "    ind = np.random.choice(X.shape[0], batch_size)\n",
        "    loss[i] = compute_loss(X, y, w)\n",
        "    visualize(X[ind,:], y[ind], w, loss)\n",
        "\n",
        "\n",
        "    z = #<YOUR CODE>\n",
        "    w = #<YOUR CODE>\n",
        "\n",
        "visualize(X, y, w, loss)\n",
        "plt.clf()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}