{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uyMZFJaS2P-B"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OWWP7w72P-G"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler, PolynomialFeatures\n",
        "from sklearn.datasets import make_circles, make_classification, make_blobs\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "from matplotlib.colors import ListedColormap\n",
        "from matplotlib import colors\n",
        "\n",
        "cm_bright = ListedColormap(['purple', 'gold'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OiGjaxPQ2P-L"
      },
      "source": [
        "## Linear SVM reminder\n",
        "\n",
        "Let's look at binary classification problem. Training samples are given by $\\{(x_n, y_n)\\}_{n=1}^N$, where $N$ — number of objects, $\\boldsymbol x_n \\in \\mathbb{R}^d$ — feature vector of object, $y_n \\in \\{+1, -1\\}$ — class of object.\n",
        "\n",
        "SVM trains model for separating hyperplane:\n",
        "$$f(\\boldsymbol x) = \\boldsymbol w^T \\boldsymbol x + b$$\n",
        "Parameters of model — vector of weights $\\boldsymbol w \\in \\mathbb{R}^d$ and bias $b \\in \\mathbb{R}$.\n",
        "\n",
        "Training is done by solving optimization problem:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\frac{1}{2} \\| \\boldsymbol w \\|^2 + C \\sum_{n=1}^N \\xi_n \\to \\min_{\\boldsymbol w, \\boldsymbol \\xi, b} \\\\\n",
        "    \\text{s.t.: } \\quad y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1 - \\xi_n, \\quad \\xi_n \\geq 0, \\quad \\forall n=1,\\dots,N\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Constraint $y_n (\\boldsymbol w^T \\boldsymbol x_n + b) \\geq 1$ assures that objects are correctly classified by separating hyperplane. Since in practice the sample could not be linearly separable the slack variables $\\xi_n$ are introduced , which weakens condition of right classification. $\\| \\boldsymbol w \\|^2$ penalise small width of margin,  $\\sum_n \\xi_n$ penalise weakens of constraints.\n",
        "\n",
        "The solution of optimization problem is given by $(\\boldsymbol w_{\\star}, \\boldsymbol \\xi_{\\star}, b_{\\star})$, some of the constraints become active, i.e. become a exact equality:\n",
        "$$\\quad y_n (\\boldsymbol w_{\\star}^T \\boldsymbol x_n + b_{\\star}) = 1 - \\xi_{\\star,n}$$\n",
        "Objects, corresponding to active constraints called $\\textbf{support vectors}$.\n",
        "\n",
        "\n",
        "Hyperparameter $C$ is responsible of balancing the width of margin and errors, made by classifier. It shows the generalizing property of the separating hyperplane - big values of $C$ corresponds to less generalizing ability and can lead to overfitting, if the data is well described by linear model. To select $C$ one must do cross-validation on validation set to find the best value.\n",
        "\n",
        "### Implementation\n",
        "\n",
        "There are two implementations of linear SVM in sklearn : [LinearSVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC) and [SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *linear* kernel. They build on different libraries, with solve optimization problem *liblinear* in first case and *libsvm* in second.\n",
        "\n",
        "Here we will use [sklearn.svm.SVC](http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC) with *kernel='linear'*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWZcj-eU2P-L"
      },
      "source": [
        "We generate data samples with:\n",
        "- linearly separable classes\n",
        "- with well separable classes, but not linearly\n",
        "- with non separable classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZQxR0ET2P-M"
      },
      "source": [
        "### Plotting function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uMUt7IW65IuF"
      },
      "outputs": [],
      "source": [
        "def fit_and_plot(X, y, model, Nx=200, Ny=200):\n",
        "  # Splitting the dataset and fitting on the train part\n",
        "  X_train, X_test, y_train, y_test = \\\n",
        "    train_test_split(X, y, test_size=.4, random_state=42)\n",
        "\n",
        "  model.fit(X_train, y_train)\n",
        "\n",
        "  # Plotting the `|margin| < 1` band\n",
        "  grid_x0 = np.linspace(X[:,0].min() - 0.5, X[:,0].max() + 0.5, Nx)\n",
        "  grid_x1 = np.linspace(X[:,1].min() - 0.5, X[:,1].max() + 0.5, Ny)\n",
        "\n",
        "  xx0, xx1 = np.meshgrid(grid_x0, grid_x1)\n",
        "  if hasattr(model, \"decision_function\"):\n",
        "    zz = model.decision_function(\n",
        "        np.c_[xx0.ravel(), xx1.ravel()]\n",
        "    ).reshape(xx0.shape)\n",
        "  else:\n",
        "    zz = model.predict_proba(\n",
        "        np.c_[xx0.ravel(), xx1.ravel()]\n",
        "    )[:,1].reshape(xx0.shape)\n",
        "\n",
        "  plt.contourf(xx0, xx1, zz, cmap=plt.cm.RdBu, alpha=.8, linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
        "  plt.contour(xx0, xx1, zz, colors=['k', 'k', 'k'], linestyles=['--', '-', '--'], levels=[-1, 0, 1])\n",
        "  plt.pcolormesh(xx0, xx1, zz,norm=colors.Normalize(0., 1.), zorder=0, alpha=0.3)\n",
        "\n",
        "  # Highlighting support vectors\n",
        "  if hasattr(model, \"named_steps\") and 'svc' in model.named_steps:\n",
        "    sv = model[:-1].inverse_transform(\n",
        "        model.named_steps['svc'].support_vectors_\n",
        "    )\n",
        "  elif hasattr(model, \"support_vectors_\"):\n",
        "    sv = model.support_vectors_\n",
        "  else:\n",
        "    sv = None\n",
        "  if sv is not None:\n",
        "    plt.scatter(*sv.T, s=180, facecolors='none', zorder=10, edgecolors='black', linewidths=0.5, label='support vectors')\n",
        "\n",
        "  # Plotting the data points\n",
        "  plt.scatter(*X_train.T, c=y_train, cmap=cm_bright, alpha=0.5, s=36, label='Train data')\n",
        "  plt.scatter(*X_test.T, c=y_test, cmap=cm_bright, alpha=1, s=36, marker='<', label='Test data')\n",
        "\n",
        "  # Tweaking the plot a bit\n",
        "  plt.xlim(xx0.min(), xx0.max())\n",
        "  plt.ylim(xx1.min(), xx1.max())\n",
        "  plt.xlabel(r\"$x_0$\")\n",
        "  plt.ylabel(r\"$x_1$\")\n",
        "  plt.legend()\n",
        "  return (model.predict(X_test) == y_test).mean()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4FX5517Ecx-"
      },
      "source": [
        "### Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MqvhhCxv9lQ7"
      },
      "outputs": [],
      "source": [
        "datasets = []\n",
        "\n",
        "# Linearly separable datset:\n",
        "datasets.append(\n",
        "    make_blobs(n_samples=100, n_features=2, centers=2, cluster_std=1.5, random_state=42)\n",
        ")\n",
        "\n",
        "# Linearly inseparable:\n",
        "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2,\n",
        "                               random_state=231, n_clusters_per_class=1)\n",
        "rng = np.random.RandomState(2)\n",
        "X += 15 * rng.uniform(size=X.shape)\n",
        "datasets.append((X, y))\n",
        "\n",
        "# Circles:\n",
        "datasets.append(make_circles(n_samples=100, noise=0.05, random_state=42))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k18ULei0Egho"
      },
      "source": [
        "### Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CWJ0cNEDOgK"
      },
      "outputs": [],
      "source": [
        "X, y = datasets[1] # Try datasets 1 and 2 as well\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(C=100., kernel='linear') # Play around with the value of C\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Kqhrb50H-Xw"
      },
      "source": [
        "Let's add PolynomialFeatures transformer to the example above.\n",
        "\n",
        "Note: sklearn's PolynomialFeatures transformer doesn't have an `inverse_transform` method defined, so our plotting code won't work. A small hack to fix this:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets[0]\n",
        "\n",
        "poly = PolynomialFeatures(2, include_bias=False)\n",
        "poly.inverse_transform = lambda X: X[:,:2]\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    poly,\n",
        "    SVC(C=1000., kernel='linear')\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ],
      "metadata": {
        "id": "EPzIDjtAJo4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAlibfBs2P-T"
      },
      "source": [
        "### Number of support vectors\n",
        "\n",
        "How does the number of support vectors depend on C on different datasets?\n",
        "\n",
        "Let's extract info about support vectors from SVC and plot a graph, representing Number of Support Vectors VS value of C for all 3 datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8-3EQWL2P-f"
      },
      "outputs": [],
      "source": [
        "C_values = np.logspace(-4, 4, 50, base = 10)\n",
        "\n",
        "\n",
        "\n",
        "n_support = {}\n",
        "for dataset, label in zip(datasets, ['sep', 'insep', 'circles']):\n",
        "  X, y = dataset\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4, random_state=42)\n",
        "  n_support[label] = []\n",
        "  for C in C_values:\n",
        "\n",
        "    model = make_pipeline(\n",
        "      StandardScaler(),\n",
        "      poly,\n",
        "      SVC(C=C, kernel='linear')\n",
        "       ).fit(X_train, y_train)\n",
        "    n_support[label].append(model[-1].n_support_.sum())\n",
        "for label in n_support:\n",
        "  plt.plot(C_values, n_support[label], label=label)\n",
        "\n",
        "plt.xscale('log')\n",
        "plt.ylabel('Number of Support Vectors')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPpzVH6d2P-r"
      },
      "source": [
        "## Kernel SVM\n",
        "\n",
        "![](http://i.imgur.com/bJAzRCt.png)\n",
        "\n",
        "Linaer SVM problem, covered above, is usually called direct optimization problem. Any direct problem has $\\textbf{dual}$ problem and in some cases optimums of both problems coincide.\n",
        "\n",
        "\n",
        "Dual problem for SVM is:\n",
        "$$\n",
        "\\begin{gather}\n",
        "    \\sum_{n} \\alpha_n - \\frac{1}{2}\\sum_{n}\\sum_{n'} \\alpha_{n}\\alpha_{n'} y_{n}y_{n'} x_{n}^Tx_{n'} \\to \\max_{\\alpha} \\\\\n",
        "    \\begin{aligned}\n",
        "        \\text{s.t. } \\quad  \n",
        "        & 0 \\le \\alpha_n \\le C, \\quad \\forall n = 1, \\dots, N \\\\\n",
        "        & \\sum_{n} \\alpha_n y_n = 0\n",
        "    \\end{aligned}\n",
        "\\end{gather}\n",
        "$$\n",
        "\n",
        "Vector of dual variables is being optimized $\\alpha_n$. Object $x_n$ is a SV, if $\\alpha_n > 0$.\n",
        "\n",
        "The predicted label is given by:\n",
        "$$\\hat{y}(x) = \\text{sign}\\left(\\sum_{n}\\alpha_{n}y_{n}x^Tx_{n} + b\\right).$$\n",
        "\n",
        "#### Kernel trick\n",
        "Notice, that dual problem has features only as a scalar product $x^Tx'$. This observation helps us to perform kernel trick - implicitly change feature space. Instead of calculating $\\phi(\\boldsymbol x)$ (as we did before) we will compute scalar product $k(\\boldsymbol x, \\boldsymbol x')$ called $\\textbf{kernel}$ and plug it insted of $x^Tx'$ above."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-XN4NVvO2P-r"
      },
      "source": [
        "Try different SVM kernels and plot pictures as above, look what other parameters kernels have and how they affect results.\n",
        "- polynomial: $k(x, x') = (\\gamma x^Tx' + r)^d$ with different $d = 2,3,\\dots$\n",
        "- Gaussian RBF: $k(x, x') = \\exp(-\\gamma\\|x - x'\\|^2)$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tbldw45VKVTp"
      },
      "outputs": [],
      "source": [
        "X, y = datasets[2]\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(C=100., kernel='linear')\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = datasets[2]\n",
        "\n",
        "model = make_pipeline(\n",
        "    StandardScaler(),\n",
        "    SVC(#...\n",
        "    )\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "fit_and_plot(X, y, model)"
      ],
      "metadata": {
        "id": "I8-h4sHzZmKs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparison with logistic regression and KNN"
      ],
      "metadata": {
        "id": "hGuMcAfXuRml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "models = {'SVM linear' : SVC(C=100., kernel='linear'),\n",
        "          'SVM linear, poly feat': make_pipeline(\n",
        "            StandardScaler(),\n",
        "            poly,\n",
        "            SVC(C=1000., kernel='linear')\n",
        "        ),\n",
        "          'SVM RBF kernel': SVC(C=1000., kernel='rbf'),\n",
        "          'Log Reg': LogisticRegression(),\n",
        "          'Log Reg, poly feat': make_pipeline(\n",
        "            StandardScaler(),\n",
        "            poly,\n",
        "            LogisticRegression()\n",
        "        ),\n",
        "          'KNN' : KNeighborsClassifier()}\n",
        "\n",
        "plt.figure(figsize=(24, 18))\n",
        "i = 1\n",
        "for (X, y) in datasets:\n",
        "  for model in models:\n",
        "\n",
        "    plt.subplot(len(datasets), len(models), i)\n",
        "    acc = fit_and_plot(X, y, models[model])\n",
        "    plt.title(model + ' ' + str(np.round(acc, 3)))\n",
        "    i+=1"
      ],
      "metadata": {
        "id": "EXr1m4ZRuhk2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}