{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bJ2JqbrfLfj"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDvcL1FjsZLY"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import trange, tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHoDZiwJsHlW"
      },
      "source": [
        "## y = wx + b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k8DvsrrgfPxC"
      },
      "source": [
        "Let's start with a toy 1D problem, where the true dependence is\n",
        "$$y=w\\cdot x+b+\\text{noise}$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKRgT8hyxRm_"
      },
      "outputs": [],
      "source": [
        "def linear_function(x):\n",
        "  return 0.33 * x + 8.3\n",
        "\n",
        "def gen_dataset(N, func, lims=(-1., 1.), noise_lvl=0.2):\n",
        "  x = np.random.uniform(*lims, size=N)\n",
        "  y = func(x) + noise_lvl * np.random.normal(size=x.shape)\n",
        "  return x[:,None], y\n",
        "\n",
        "X, y = gen_dataset(50, linear_function)\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x))\n",
        "plt.scatter(X, y);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bG5zspXRrYWL"
      },
      "source": [
        "### `LinearRegression` from `sklearn`"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`LinearRegression` will find analytical solution for linear regression with the MSE loss"
      ],
      "metadata": {
        "id": "bzVzXFx3FXho"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "92YYaRU9e2jz"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, linear_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend()\n",
        "\n",
        "print(model.coef_, model.intercept_)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if analytical solution, how to get it - [link](https://datamapu.com/posts/classical_ml/linear_regression_example/)"
      ],
      "metadata": {
        "id": "sO9P6UbtUY9S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w_sol = ((y - y.mean()) * X.flatten()).sum() / ((X.flatten() - X.mean()) * X.flatten()).sum()\n",
        "b_sol = y.mean() - w_sol * X.mean()\n",
        "\n",
        "assert np.allclose(w_sol, model.coef_)\n",
        "assert np.allclose(b_sol, model.intercept_)\n",
        "\n",
        "w_sol, b_sol"
      ],
      "metadata": {
        "id": "e5025HgrSFiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WGS--_XrSe9"
      },
      "source": [
        "### Sidenote: making contour plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ndqMFPx6oTpP"
      },
      "outputs": [],
      "source": [
        "### Sidenote: making contour plots (level maps)\n",
        "\n",
        "plt.contourf(\n",
        "    [[0., 1., 2.], # matrix of X\n",
        "     [0., 1., 2.],\n",
        "     [0., 1., 2.]],\n",
        "    [[ 0.,  0.,  0.], # matrix of Y\n",
        "     [10., 10., 10.],\n",
        "     [20., 20., 20.]],\n",
        "    [[-1., 0., -1.], # matrix of Z\n",
        "     [ 0., 1.,  0.],\n",
        "     [-1., 0., -1.]]\n",
        ");"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQs61KUCqV5G"
      },
      "outputs": [],
      "source": [
        "### 2D matrices of X and Y (as above) can be\n",
        "### created from 1D vectors using np.meshgrid:\n",
        "\n",
        "for i in np.meshgrid([0., 1., 2], [0., 10., 20.]):\n",
        "  print(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-PAHAotrkJV"
      },
      "source": [
        "### MSE as a function of model parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFQzedCIst18"
      },
      "source": [
        "Let's see what MSE looks like as a function of model parameters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MNT3O5soiWwb"
      },
      "outputs": [],
      "source": [
        "# Creating a grid of model parameter values:\n",
        "ww, bb = np.meshgrid(\n",
        "    np.linspace(-10., 10., 50),\n",
        "    np.linspace(-5., 15., 50)\n",
        ")\n",
        "\n",
        "# Calculating the map of MSE values on the grid defined above, i.e.\n",
        "# for each (w, b) in (ww, bb) calculating MSE for the model y = w * x + b.\n",
        "MSE_map = ((X[..., np.newaxis] * ww[np.newaxis,...] + bb[np.newaxis,...] - y[..., np.newaxis, np.newaxis]) ** 2).mean(axis=0)\n",
        "\n",
        "# Automatic checks\n",
        "assert MSE_map.shape == ww.shape\n",
        "for i in [0, -1]:\n",
        "  for j in [0, -1]:\n",
        "    assert np.isclose(\n",
        "        MSE_map[i, j],\n",
        "        ((ww[i, j] * X.ravel() + bb[i, j] - y)**2).mean()\n",
        "    ), f'assert failed for point {i, j}'\n",
        "\n",
        "# Plotting:\n",
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
        "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oKmUx_pxFLWV"
      },
      "source": [
        "## Gradient descent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gGSJnwrRFLWV"
      },
      "source": [
        "\n",
        "$$\\text{MSE}(w, b)=\\frac{1}{N}\\sum_{i=1}^N((x w + b) - y)^2$$\n",
        "\n",
        "Instead of minimizing it analytically, we can use numeric optimization with gradient descent. I.e. do the following procedure iteratively:\n",
        "$$w\\leftarrow w-\\alpha\\cdot\\frac{\\partial\\text{MSE}(w, b)}{\\partial w},$$\n",
        "for some constant *learning rate* $\\alpha$.\n",
        "\n",
        "The same can be done for $b$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check the model trajectory in GD"
      ],
      "metadata": {
        "id": "ICL9hUzuEg6k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial parameters\n",
        "w = -4\n",
        "b = -4\n",
        "\n",
        "loss_values, w_values, b_values = [], [w], [b]\n",
        "\n",
        "learning_rate = 0.1\n",
        "\n",
        "# Training loop\n",
        "for _ in trange(1000):\n",
        "  # Your turn: calculate the gradient of MSE with respect to w and b\n",
        "  gradient_w = 2 * (((X * w).flatten() + b - y) * X.flatten()).sum() / len(X)#<YOUR CODE>\n",
        "  gradient_b = 2 * (((X * w).flatten() + b - y)).sum() / len(X)\n",
        "\n",
        "\n",
        "  # Gradient descent step\n",
        "  w -= learning_rate * gradient_w\n",
        "  b -= learning_rate * gradient_b\n",
        "\n",
        "\n",
        "\n",
        "  # Calculate and record the new loss value + w and b\n",
        "  loss_values.append(\n",
        "      ((X * w + b - y)**2).mean()\n",
        "  )\n",
        "  w_values.append(w)\n",
        "  b_values.append(b)\n",
        "\n",
        "# Plotting the evolution of loss values\n",
        "plt.plot(loss_values)\n",
        "plt.title('Loss');\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the solution\n",
        "x = np.linspace(-1, 1, 101)\n",
        "\n",
        "plt.plot(x, linear_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x,\n",
        "         x * w + b, label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "HjBryloqEvBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
        "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange')\n",
        "\n",
        "plt.plot(w_values, b_values, c='red', marker='.')\n"
      ],
      "metadata": {
        "id": "kIe2gBiTMqA7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's check stohastic gradient descent (the gradient is computet using few random samples)"
      ],
      "metadata": {
        "id": "Rq1Mfr_bNlE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Initial parameters\n",
        "w = -4\n",
        "b = -4\n",
        "\n",
        "loss_values, w_values, b_values = [], [w], [b]\n",
        "\n",
        "learning_rate = 0.1\n",
        "batch_size = 2\n",
        "\n",
        "# Training loop\n",
        "for _ in trange(1000):\n",
        "  # Your turn: calculate the gradient of MSE with respect to w and b\n",
        "  idx = np.random.choice(len(X), size=batch_size, replace=False)\n",
        "\n",
        "  gradient_w = (((X[idx] * w).flatten() + b - y[idx]) * X[idx].flatten()).sum() / len(X[idx])#<YOUR CODE>\n",
        "  gradient_b = (((X[idx] * w).flatten() + b - y[idx])).sum() / len(X[idx])\n",
        "\n",
        "  # Gradient descent step\n",
        "  w -= learning_rate * gradient_w\n",
        "  b -= learning_rate * gradient_b\n",
        "\n",
        "  # Calculate and record the new loss value + w and b\n",
        "  loss_values.append(\n",
        "      ((X * w + b - y)**2).mean()\n",
        "  )\n",
        "  w_values.append(w)\n",
        "  b_values.append(b)\n",
        "\n",
        "# Plotting the evolution of loss values\n",
        "plt.plot(loss_values)\n",
        "plt.title('Loss');\n",
        "plt.show()\n",
        "\n",
        "\n",
        "# Plotting the solution\n",
        "x = np.linspace(-1, 1, 101)\n",
        "\n",
        "plt.plot(x, linear_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x,\n",
        "         x * w + b, label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ],
      "metadata": {
        "id": "8HU4wnovNpZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(6, 5), dpi=100)\n",
        "plt.colorbar(plt.contourf(ww, bb, MSE_map, levels=30))\n",
        "plt.scatter(model.coef_, model.intercept_, marker='*', s=150, c='orange')\n",
        "\n",
        "plt.plot(w_values, b_values, c='red', marker='.')\n"
      ],
      "metadata": {
        "id": "xcxSCQYIODLe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA_a_UsFsYPR"
      },
      "source": [
        "## Polynomial fit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gCSdUOg__u0n"
      },
      "source": [
        "Now let's take some arbitrary function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6X2splyeIVH"
      },
      "outputs": [],
      "source": [
        "def true_function(x):\n",
        "  return np.sin(3 * x + 0.8) + np.sin(1. / (x + 1.23))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPgBZq4l_0Ex"
      },
      "source": [
        "Obviously, we won't get a good fit with an ordinary linear regression:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KmSBngOE0JdP"
      },
      "outputs": [],
      "source": [
        "model = LinearRegression()\n",
        "X, y = gen_dataset(25, true_function)\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXhZyytaANX2"
      },
      "source": [
        "### `PolynomialFeatures` and pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbESfwRuAZD7"
      },
      "source": [
        "Even though our design matrix has only one column:\n",
        "$$X=\n",
        "\\begin{pmatrix}\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots \\\\\n",
        "x_N\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "we can expand it with powers of $x$ to fit a polynomial:\n",
        "$$X'=\n",
        "\\begin{pmatrix}\n",
        "x_1 & (x_1)^2 & \\ldots & (x_1)^k \\\\\n",
        "x_2 & (x_2)^2 & \\ldots & (x_2)^k \\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "x_N & (x_N)^2 & \\ldots & (x_N)^k\n",
        "\\end{pmatrix},\n",
        "$$\n",
        "\n",
        "such that:\n",
        "\n",
        "$$\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2\\to \\underset{w}{\\text{min}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wBX4E80aDfBv"
      },
      "source": [
        "This functionality is implemented in `sklearn.preprocessing.PolynomialFeatures`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tnvqh81sDeJb"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly_expand = PolynomialFeatures(3)\n",
        "poly_expand.fit_transform(np.arange(5)[:,None])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD4hq4nVE0WO"
      },
      "source": [
        "One can combine `PolynomialFeatures` (and any other transformers) along with the model into a single pipeline:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jtNLILIP25-U"
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import make_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXG8osw43CI-"
      },
      "outputs": [],
      "source": [
        "# The first parameter is the power of expansion. Try playing around with it.\n",
        "poly_expand = PolynomialFeatures(5, include_bias=False)\n",
        "linear_model = LinearRegression()\n",
        "model = make_pipeline(\n",
        "    poly_expand, linear_model\n",
        ")\n",
        "\n",
        "model.fit(X, y)\n",
        "\n",
        "x = np.linspace(-1, 1, 101)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x, model.predict(x[:,None]), label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiWLsor3c2hp"
      },
      "source": [
        "Now we want to plot 2D projections of MSE as a function of model parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UTXh8fe9-gXE"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Combine the weights and the bias into a single parameter vector\n",
        "solution = np.concatenate([linear_model.coef_, [linear_model.intercept_]])\n",
        "\n",
        "# Calculate the power expansion of the features\n",
        "X_expanded = np.concatenate([\n",
        "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
        "], axis=1)\n",
        "\n",
        "\n",
        "X_expanded.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xZFirLY3hHj"
      },
      "outputs": [],
      "source": [
        "# We'll plot a large matrix of plots, so let's create\n",
        "# a 16 by 16 inch canvas\n",
        "plt.figure(figsize=(16, 16))\n",
        "\n",
        "# We'll loop over all pairs of weights\n",
        "i_img = 0\n",
        "powers = list(range(1, X_expanded.shape[1])) + [0]\n",
        "for dim1 in trange(len(solution)):\n",
        "  for dim2 in range(len(solution)):\n",
        "\n",
        "    #Preparing the subplot\n",
        "    i_img += 1\n",
        "    plt.subplot(len(solution), len(solution), i_img)\n",
        "    plt.xticks([], [])\n",
        "    plt.yticks([], [])\n",
        "    if (i_img-1)%X_expanded.shape[1]  == 0: plt.ylabel(powers[(i_img-1)//X_expanded.shape[1]])\n",
        "    if (i_img-1)//X_expanded.shape[1]  == X_expanded.shape[1] -1: plt.xlabel(powers[(i_img-1)%X_expanded.shape[1]])\n",
        "\n",
        "    # Skip the diagonal\n",
        "    if dim1 == dim2: continue\n",
        "\n",
        "    # Create the grid of parameter values\n",
        "    ww1, ww2 = np.meshgrid(\n",
        "        np.linspace(solution[dim1] - 1000., solution[dim1] + 1000., 50),\n",
        "        np.linspace(solution[dim2] - 1000., solution[dim2] + 1000., 50),\n",
        "    )\n",
        "\n",
        "    # To calculate the map of MSE values, let's first\n",
        "    # create `param_grid` - a 3D array of parameter values of the\n",
        "    # following shape: (len(solution), ww1.shape[0], ww1.shape[1])\n",
        "    #\n",
        "    # I.e. `param_grid[i, :, :]` should equal to:\n",
        "    #     `ww1` if `i` equals `dim1`;\n",
        "    #     `ww2` if `i` equals `dim2`;\n",
        "    #     `solution[i]` otherwise.\n",
        "\n",
        "    param_grid = np.empty(shape= (len(solution),) + ww1.shape, dtype=float)\n",
        "    param_grid[:] = solution[:,None,None]\n",
        "    param_grid[dim1] = ww1\n",
        "    param_grid[dim2] = ww2\n",
        "\n",
        "    # Automatic checks\n",
        "    assert param_grid.shape == (len(solution),) + ww1.shape\n",
        "    assert np.allclose(param_grid[dim1], ww1)\n",
        "    assert np.allclose(param_grid[dim2], ww2)\n",
        "    assert all(\n",
        "        np.allclose(param_grid[i], solution[i])\n",
        "        for i in range(len(solution)) if i not in (dim1, dim2)\n",
        "    )\n",
        "\n",
        "    # Now it's time to calculate the MSE map, i.e. for each grid\n",
        "    # element (i, j), you want `MSE_map[i, j]` to be equal to the MSE\n",
        "    # for the model defined by parameters `param_grid[:, i, j]`.\n",
        "    MSE_map = ((np.squeeze((X_expanded @ param_grid[..., np.newaxis].swapaxes(0, 2)).swapaxes(0, 2)) - y[..., np.newaxis, np.newaxis])**2).mean(axis=0)\n",
        "\n",
        "    # Automatic checks\n",
        "    assert MSE_map.shape == ww1.shape\n",
        "    for i in [0, -1]:\n",
        "      for j in [0, -1]:\n",
        "        assert np.isclose(\n",
        "            ((X_expanded @ param_grid[:, i, j] - y)**2).mean(),\n",
        "            MSE_map[i, j]\n",
        "        ), f'Check failed for point {i, j}'\n",
        "\n",
        "\n",
        "    plt.contourf(ww1, ww2, MSE_map, levels=10);\n",
        "    plt.scatter(solution[dim1], solution[dim2], marker='*', s=30, c='orange')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUbyj4uLMsMp"
      },
      "source": [
        "Note the relation between the amount of overfitting and correlation between parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-ZaSJCifkV"
      },
      "source": [
        "For gradient descent:\n",
        "\n",
        "$$\\text{MSE}(w)=\\frac{1}{N}\\left\\Vert X'\\cdot w - y\\right\\Vert^2$$\n",
        "\n",
        "The procedure is the same, but $w$ is a vector:\n",
        "$$w\\leftarrow w-\\alpha\\cdot\\frac{\\partial\\text{MSE}(w)}{\\partial w}$$\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2AERh5jc-tBT"
      },
      "outputs": [],
      "source": [
        "X_expanded = np.concatenate([\n",
        "    poly_expand.transform(X), np.ones(shape=(len(X), 1))\n",
        "], axis=1)\n",
        "\n",
        "\n",
        "# Initialize the model parameters with zeros\n",
        "w = np.zeros(dtype=float, shape=X_expanded.shape[1])\n",
        "\n",
        "loss_values = []\n",
        "learning_rate = 0.01\n",
        "\n",
        "\n",
        "for _ in trange(1000):\n",
        "  # Your turn: calculate the gradient of MSE with respect to w:\n",
        "  gradient = 2 * ((X_expanded @ w - y) @ X_expanded).flatten()#<YOUR CODE>\n",
        "\n",
        "  # Automatic checks\n",
        "  assert gradient.shape == w.shape\n",
        "  assert (\n",
        "      ((X_expanded @ w - y)**2).mean() >\n",
        "      ((X_expanded @ (w - learning_rate * gradient) - y)**2).mean()\n",
        "  )\n",
        "\n",
        "\n",
        "  w -= learning_rate * gradient\n",
        "\n",
        "\n",
        "  loss_values.append(\n",
        "      ((X_expanded @ w - y)**2).mean()\n",
        "  )\n",
        "\n",
        "plt.plot(loss_values)\n",
        "plt.title('Loss');\n",
        "plt.show()\n",
        "x = np.linspace(-1, 1, 101)\n",
        "x_expanded = np.concatenate([\n",
        "    poly_expand.transform(x[:,None]),\n",
        "    np.ones(shape=(len(x), 1))\n",
        "], axis=1)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y);\n",
        "plt.plot(x,\n",
        "         x_expanded @ w, label='prediction')\n",
        "plt.ylim(y.min() - 0.5, y.max() + 0.5)\n",
        "plt.legend();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBenTP_0oDB-"
      },
      "source": [
        "Did you notice that numeric solution is less prone to overfitting? Some intuition for that can be found in this post: https://distill.pub/2017/momentum/ (though not explicitly)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xz42oYY9NK8X"
      },
      "source": [
        "## Here comes the overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d-4VlHXJNK8X"
      },
      "source": [
        "What is going to happen if we increase the number of powers we use?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cb99I_IjNK8X"
      },
      "outputs": [],
      "source": [
        "X, y = gen_dataset(25, true_function)\n",
        "\n",
        "# Define the set of powers to be used generating the features\n",
        "set_of_powers =[ 1, 5, 15, 25 ]\n",
        "\n",
        "# Plotting the ground truth\n",
        "plt.figure(figsize=(15,6))\n",
        "x = np.linspace(-1, 1, 200)\n",
        "plt.plot(x, true_function(x), label='true function')\n",
        "plt.scatter(X, y,marker='*', s = 100);\n",
        "plt.ylim(-2, 2)\n",
        "\n",
        "# Adding plots for every single value in the set of powers using the pipeline\n",
        "for d in set_of_powers:\n",
        "    poly_expand = PolynomialFeatures(d, include_bias=False)\n",
        "    model = make_pipeline(poly_expand, LinearRegression())\n",
        "    model.fit(X, y)\n",
        "    plt.plot(x, model.predict(x[:,None]),linewidth=2, label=\"$d=%d$\" % d)\n",
        "plt.legend(loc = 1)\n",
        "plt.title\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGm2N8J4NK8X"
      },
      "source": [
        "Using `sklearn` LinearRegression, it is possible to fit every single point, but it is not the objective we try to achieve."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}